%Εκπαιδεύουμε τους ταξινομητές SVM χρησιμοποιώντας πυρήνα Gaussian

%{
Αυτό το παράδειγμα δείχνει πώς να δημιουργήσουμε έναν μη γραμμικό ταξινομητή με τη λειτουργία πυρήνα Gaussian.
Πρώτα, δημιουργούμε μια κατηγορία σημείων μέσα στο δίσκο μονάδας σε δύο διαστάσεις και μια άλλη κλάση σημείων
στον δακτύλιο από την ακτίνα 1 έως την ακτίνα 2. 
Στη συνέχεια, δημιουργεί έναν ταξινομητή με βάση τα δεδομένα
με τον πυρήνα λειτουργίας ακτινικής βάσης Gaussian. 
Ο προεπιλεγμένος γραμμικός ταξινομητής είναι προφανώς ακατάλληλος
για αυτό το πρόβλημα, καθώς το μοντέλο είναι κυκλικά συμμετρικό. Ορίζουμε την παράμετρο περιορισμού πλαισίου σε Inf
για να κάνουμε μια αυστηρή ταξινόμηση, που σημαίνει ότι δεν υπάρχουν εσφαλμένα ταξινομημένα σημεία. 
Άλλες λειτουργίες του πυρήνα μπορεί να μην λειτουργεί με αυτόν τον αυστηρό περιορισμό του κουτιού, καθώς ενδέχεται να μην είναι σε θέση να παράσχει αυστηρή ταξινόμηση.
Ακόμα κι αν ο ταξινομητής rbf μπορεί να διαχωρίσει τις κλάσεις, το αποτέλεσμα μπορεί να είναι υπερβολικό.

Δημιουργούμε 100 πόντους που κατανέμονται ομοιόμορφα στο δίσκο μονάδας. Για να το κάνουμε αυτό, δημιουργούμε μια ακτίνα r ως τετραγωνική ρίζα
μιας ομοιόμορφης τυχαίας μεταβλητής, δημιουργούμε μια γωνία t ομοιόμορφα στο (0,) και τοποθετούμε το σημείο στο (r cos (t), r sin (t)).
%}
rng(1); % Για αναπαραγωγιμότητα
r = sqrt(rand(100,1)); % Ακτίνα κύκλου
t = 2*pi*rand(100,1);  % Γωνία
data1 = [r.*cos(t), r.*sin(t)]; % Πόντοι

%{
Δημιούργούμε 100 πόντους ομοιόμορφα κατανεμημένους στο δακτύλιο. 
Η ακτίνα είναι πάλι ανάλογη με μια τετραγωνική ρίζα,
αυτή τη φορά μια τετραγωνική ρίζα της ομοιόμορφης κατανομής από 1 έως 4.
%}
r2 = sqrt(3*rand(100,1)+1); % Ακτίνα κύκλου
t2 = 2*pi*rand(100,1);      % Γωνία
data2 = [r2.*cos(t2), r2.*sin(t2)]; % Πόντοι

%Σχεδιάζουμε τα σημεία και τους κύκλους των ακτίνων 1 και 2 για σύγκριση
figure;
plot(data1(:,1),data1(:,2),'r.','MarkerSize',15)
hold on
plot(data2(:,1),data2(:,2),'b.','MarkerSize',15)
ezpolar(@(x)1);ezpolar(@(x)2);
axis equal
hold off

%Τοποθέτηούμε τα δεδομένα σε έναν πίνακα και δημιουργήστε ένα διάνυσμα ταξινομήσεων
data3 = [data1;data2];
theclass = ones(200,1);
theclass(1:100) = -1;

%{
Εκπαίδεύουμε έναν ταξινομητή SVM με το KernelFunction σε «rbf» και το BoxConstraint σε Inf.
Σχεδιάζουμε το όριο απόφασης και επισημάνετε τα διανύσματα υποστήριξης
%}
%Εκπαίδεύουμε τον ταξινομητή SVM
cl = fitcsvm(data3,theclass,'KernelFunction','rbf',...
    'BoxConstraint',Inf,'ClassNames',[-1,1]);

% Προβλέπουμε το σκορ στο πλέγμα
d = 0.02;
[x1Grid,x2Grid] = meshgrid(min(data3(:,1)):d:max(data3(:,1)),...
    min(data3(:,2)):d:max(data3(:,2)));
xGrid = [x1Grid(:),x2Grid(:)];
[~,scores] = predict(cl,xGrid);

% Σχεδιάζουμε τα δεδομένα και το όριο της απόφασης
figure;
h(1:2) = gscatter(data3(:,1),data3(:,2),theclass,'rb','.');
hold on
ezpolar(@(x)1);
h(3) = plot(data3(cl.IsSupportVector,1),data3(cl.IsSupportVector,2),'ko');
contour(x1Grid,x2Grid,reshape(scores(:,2),size(x1Grid)),[0 0],'k');
legend(h,{'-1','+1','Support Vectors'});
axis equal
hold off

%Το fitcsvm δημιουργεί έναν ταξινομητή που βρίσκεται κοντά σε έναν κύκλο ακτίνας 1. Η διαφορά οφείλεται στα τυχαία δεδομένα εκπαίδευσης.

%{
Η εκπαίδευση με τις προεπιλεγμένες παραμέτρους κάνει ένα σχεδόν κυκλικό όριο ταξινόμησης,
αλλά ένα που ταξινομεί εσφαλμένα ορισμένα δεδομένα εκπαίδευσης. Επίσης, η προεπιλεγμένη τιμή του BoxConstraint είναι 1,
και, επομένως, υπάρχουν περισσότεροι φορείς υποστήριξης.
%}
cl2 = fitcsvm(data3,theclass,'KernelFunction','rbf');
[~,scores2] = predict(cl2,xGrid);

figure;
h(1:2) = gscatter(data3(:,1),data3(:,2),theclass,'rb','.');
hold on
ezpolar(@(x)1);
h(3) = plot(data3(cl2.IsSupportVector,1),data3(cl2.IsSupportVector,2),'ko');
contour(x1Grid,x2Grid,reshape(scores2(:,2),size(x1Grid)),[0 0],'k');
legend(h,{'-1','+1','Support Vectors'});
axis equal
hold off